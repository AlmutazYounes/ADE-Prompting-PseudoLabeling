# ADR Train ModernBERT: ADE Extraction Pipeline

A comprehensive pipeline for extracting Adverse Drug Events (ADEs) from medical notes using Bio_ClinicalBERT and Large Language Models (LLMs).

## Overview

This project implements an end-to-end solution for identifying drugs, adverse events (ADEs), and their potential relationships within clinical texts. It offers a flexible framework that combines the strengths of transformer-based models like Bio_ClinicalBERT with various LLM-based extraction techniques.

The primary goal is to evaluate and compare different approaches for ADE extraction:
1.  **Bio_ClinicalBERT (Direct LLM Training Data)**: Fine-tuning Bio_ClinicalBERT on data generated directly by an LLM.
2.  **Bio_ClinicalBERT (DSPy-Optimized LLM Training Data)**: Fine-tuning Bio_ClinicalBERT on data generated by an LLM enhanced with DSPy optimization techniques.
3.  **Direct LLM Extraction**: Using an LLM (e.g., GPT-4o-mini) directly for ADE extraction.
4.  **DSPy-Optimized LLM Extraction**: Employing DSPy to optimize prompts and few-shot examples for more robust LLM-based extraction.

The pipeline handles data loading, preprocessing, multiple extraction strategies, Bio_ClinicalBERT fine-tuning, and comprehensive evaluation, producing comparative metrics and visualizations.

## Features

-   âš•ï¸ **Medical Domain-Focused**: Specifically designed for extracting drugs and ADEs from clinical narratives.
-   ğŸ”„ **Multiple Extraction Strategies**:
    -   Direct LLM calls (OpenAI).
    -   DSPy-enhanced LLM extraction with Chain-of-Thought and potential optimization.
    -   Fine-tuned Bio_ClinicalBERT for token classification (NER).
-   âš™ï¸ **Bio_ClinicalBERT Fine-tuning**: Leverages Hugging Face Transformers for training sequence labeling models on custom NER data.
-   ğŸ“Š **Comparative Evaluation**:
    -   Evaluates all four approaches against a gold-standard dataset.
    -   Uses token-level (seqeval) metrics for BERT models and entity-level metrics for LLM approaches.
    -   Generates comparison plots (F1, Precision, Recall).
-   ğŸ’¾ **Persistent Caching**: Caches LLM extraction results to save costs and time during reruns.
-   ğŸ”§ **Configurable**: Easily modify model names, paths, and training parameters via `utils/config.py`.
-   ğŸ“ **Detailed Logging**: Provides insights into each step of the process.
-   ğŸ“¦ **Modular Utilities**: Well-structured utility modules for data transformation, dataset handling, evaluation, and extraction.

## Pipeline Workflow

The primary script, `run_all_bio_BERT.py`, orchestrates the comparison:

1.  **Load Gold Standard Data**: Loads manually annotated data (NER format) for evaluation.
2.  **Load Bio_ClinicalBERT Models**:
    -   Finds the latest fine-tuned Bio_ClinicalBERT models (one trained on direct LLM output, one on DSPy output) from `bert_ade_extractor/`.
    -   Allows specifying particular model folders via command-line arguments (`--direct-bert-folder`, `--dspy-bert-folder`).
3.  **Evaluate Fine-tuned Bio_ClinicalBERT Models**:
    -   Performs token-level evaluation (using `seqeval`) against the gold standard.
4.  **Evaluate LLM-based Approaches (Direct & DSPy)**:
    -   Loads cached LLM extractions from `analysis/llm_cache/` if available and `--overwrite-llm` is not set.
    -   Otherwise, performs new extractions using the `DirectLLMExtractor` and `ADEExtractor` (DSPy) from `utils/extraction.py`.
    -   Calculates entity-level metrics (precision, recall, F1) by comparing extracted drugs and ADEs against the gold standard.
5.  **Visualize Results**:
    -   Generates bar plots comparing F1, precision, and recall scores across all four approaches.
    -   Saves metrics and plots to a timestamped folder in `analysis/comparison_results/`.

The `BERT_pipeline.py` script is responsible for fine-tuning the Bio_ClinicalBERT models:

1.  **Load Training Data**: Loads NER-formatted data (either from `STEP_2_NER_DATA_DIRECT` or `STEP_2_NER_DATA_DSPY` as defined in `utils/config.py`). This data is typically the output of LLM extraction.
2.  **Prepare Datasets**: Splits data into training and validation sets, prepares label mappings.
3.  **Tokenize and Align Labels**: Processes raw text and entities into tokens and BIO tags, aligning them for the BERT model.
4.  **Initialize Model**: Loads a pre-trained `BERT_MODEL_NAME` (e.g., `MutazYoune/ClinicalBERT-AE-NER`) for token classification.
5.  **Train Model**: Uses Hugging Face `Trainer` with specified `TRAINING_ARGS` from `utils/config.py`.
6.  **Evaluate on Gold Data**: Evaluates the trained model on `STEP_2_GOLD_NER_DATA`.
7.  **Save Model**: Saves the fine-tuned model, tokenizer, and metrics to a timestamped subfolder in `BERT_OUTPUT_DIR`.

## Project Structure

```
ADR_train_modernBERT/
â”œâ”€â”€ analysis/
â”‚   â”œâ”€â”€ comparison_results/       # Stores results from run_all_bio_BERT.py
â”‚   â”‚   â””â”€â”€ <timestamp>_bio_clinicalbert/
â”‚   â”‚       â”œâ”€â”€ metrics.json
â”‚   â”‚       â”œâ”€â”€ f1_comparison.png
â”‚   â”‚       â””â”€â”€ prf_comparison.png
â”‚   â””â”€â”€ llm_cache/                # Cached LLM extraction outputs
â”‚       â”œâ”€â”€ llm_direct.jsonl
â”‚       â””â”€â”€ llm_dspy.jsonl
â”œâ”€â”€ bert_ade_extractor/           # Output directory for fine-tuned BERT models
â”‚   â”œâ”€â”€ direct_approach_<timestamp>/ # Model trained on direct LLM data
â”‚   â””â”€â”€ dspy_approach_<timestamp>/   # Model trained on DSPy LLM data
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ direct/                   # Data generated by direct LLM approach
â”‚   â”‚   â”œâ”€â”€ extracted_data.jsonl
â”‚   â”‚   â””â”€â”€ ner_data.jsonl
â”‚   â”œâ”€â”€ dspy/                     # Data generated by DSPy approach
â”‚   â”‚   â”œâ”€â”€ extracted_data.jsonl
â”‚   â”‚   â””â”€â”€ ner_data.jsonl
â”‚   â””â”€â”€ gold/                     # Gold standard data
â”‚       â”œâ”€â”€ gold_extracted_data.jsonl (deprecated)
â”‚       â””â”€â”€ gold_ner_data.jsonl
â”œâ”€â”€ utils/                        # Utility modules
â”‚   â”œâ”€â”€ config.py                 # Configuration settings (paths, model names, etc.)
â”‚   â”œâ”€â”€ data_transformation.py    # Functions for data conversion (e.g., to NER, BIO)
â”‚   â”œâ”€â”€ dataset.py                # ADEDatasetProcessor and PyTorch Dataset class
â”‚   â”œâ”€â”€ evaluation.py             # Metrics calculation and visualization
â”‚   â”œâ”€â”€ extraction.py             # LLM and DSPy based extraction logic
â”‚   â”œâ”€â”€ logging_utils.py          # Logging utilities (e.g. for litellm)
â”‚   â””â”€â”€ utils.py                  # General utility functions (file I/O, error handling)
â”œâ”€â”€ .env                          # Environment variables (e.g., OPENAI_API_KEY)
â”œâ”€â”€ .gitignore
â”œâ”€â”€ BERT_pipeline.py              # Script for fine-tuning Bio_ClinicalBERT models
â”œâ”€â”€ generate_gold_tokens_tags.py  # Script to generate gold standard in token/tag format (if needed)
â”œâ”€â”€ generate_training_data_pipeline.py # Main script to generate training data using LLMs
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt              # Python dependencies
â””â”€â”€ run_all_bio_BERT.py           # Main script to run all evaluations and comparisons
```

## Installation

1.  **Clone the repository:**
    ```bash
    git clone <your-repo-url>
    cd ADR_train_modernBERT
    ```

2.  **Create and activate a virtual environment:**
    ```bash
    python3 -m venv venv
    source venv/bin/activate  # On Windows: venv\Scripts\activate
    ```

3.  **Install dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

4.  **Set up your OpenAI API key:**
    Create a `.env` file in the project root with your API key:
    ```env
    OPENAI_API_KEY="your_openai_api_key_here"
    ```

## Usage

### 1. Prepare Gold Standard Data
Ensure your gold standard data is in JSONL format, where each line is a JSON object with `"text"` and `"entities"` fields. The `entities` field should be a list of dictionaries, each with `"start"`, `"end"`, and `"label"` (e.g., "DRUG", "ADE").
Place this file at the path specified by `STEP_2_GOLD_NER_DATA` in `utils/config.py` (default: `data/gold/gold_ner_data.jsonl`).

### 2. Generate Training Data using LLMs (Optional - if not already generated)
Run the `generate_training_data_pipeline.py` script if you need to generate `ner_data.jsonl` files for `direct` and `dspy` approaches. This script will:
    - Load raw texts.
    - Use `DirectLLMExtractor` to create `data/direct/extracted_data.jsonl` and then `data/direct/ner_data.jsonl`.
    - Use `ADEExtractor` (DSPy) to create `data/dspy/extracted_data.jsonl` and then `data/dspy/ner_data.jsonl`.
    ```bash
    python generate_training_data_pipeline.py
    ```

### 3. Fine-tune Bio_ClinicalBERT Models
Use `BERT_pipeline.py` to train models on the LLM-generated data.
Run this script twice, once for each approach:
    ```bash
    # Train on data from the direct LLM approach
    python BERT_pipeline.py --mode direct

    # Train on data from the DSPy LLM approach
    python BERT_pipeline.py --mode dspy
    ```
This will save fine-tuned models into subdirectories within `bert_ade_extractor/`.

### 4. Run Comprehensive Evaluation
Execute the `run_all_bio_BERT.py` script to evaluate all four approaches:
    ```bash
    python run_all_bio_BERT.py
    ```
Optional arguments:
-   `--overwrite-llm`: Force re-extraction of LLM results, ignoring cached files.
-   `--direct-bert-folder <folder_name>`: Specify a custom folder within `bert_ade_extractor/` for the "Bio_ClinicalBERT (Direct)" model (e.g., `direct_approach_YYYYMMDD_HHMMSS`).
-   `--dspy-bert-folder <folder_name>`: Specify a custom folder for the "Bio_ClinicalBERT (DSPy)" model.

The script will output metrics and generate comparison plots in `analysis/comparison_results/`.

### Example Output (`analysis/comparison_results/.../metrics.json`)
```json
{
  "Bio_ClinicalBERT (Direct)": {
    "precision": 0.77, // Example value
    "recall": 0.81,
    "f1": 0.79
  },
  "Bio_ClinicalBERT (DSPy)": { // If DSPy model was trained and found
    "precision": 0.78,
    "recall": 0.82,
    "f1": 0.80
  },
  "Direct LLM": {
    "precision": 0.84,
    "recall": 0.77,
    "f1": 0.79
  },
  "DSPy": {
    "precision": 0.86,
    "recall": 0.72,
    "f1": 0.76
  }
}
```

## Configuration (`utils/config.py`)

Key configuration options include:
-   `OPENAI_API_KEY`: Loaded from `.env`.
-   `BASE_DIR`, `DATA_DIR`: Project and data directories.
-   `STEP_2_GOLD_NER_DATA`: Path to the gold standard NER data.
-   `LLM_MODEL_NAME`: LLM to use for extraction (e.g., "gpt-4o-mini").
-   `BERT_OUTPUT_DIR`: Directory to save fine-tuned BERT models.
-   `BERT_MODEL_NAME`: Base BERT model for fine-tuning (e.g., "MutazYoune/ClinicalBERT-AE-NER").
-   `BERT_MAX_LENGTH`: Max sequence length for BERT.
-   `TRAINING_ARGS`: Dictionary of Hugging Face `TrainingArguments`.
-   Paths for intermediate data files (e.g., `STEP_1_LLM_DIRECT`, `STEP_2_NER_DATA_DSPY`).

## Key Scripts and Modules

-   **`run_all_bio_BERT.py`**: Main evaluation script. Compares fine-tuned BERT models and LLM approaches.
-   **`BERT_pipeline.py`**: Handles the fine-tuning process for Bio_ClinicalBERT models.
-   **`generate_training_data_pipeline.py`**: Generates `ner_data.jsonl` from raw text using direct LLM and DSPy LLM approaches.
-   **`utils/config.py`**: Central configuration for paths, model names, and hyperparameters.
-   **`utils/extraction.py`**:
    -   `DirectLLMExtractor`: Simple OpenAI API calls for extraction.
    -   `ADEExtractor`: DSPy module for potentially optimized extraction.
    -   `initialize_extractor()`: Factory function to get an extractor instance.
-   **`utils/data_transformation.py`**: Functions to convert data between formats (e.g., raw text + entities to NER JSONL, NER to BIO tags).
-   **`utils/dataset.py`**:
    -   `ADEDatasetProcessor`: Processes notes, extracts ADEs, and prepares data for NER/BIO formats.
    -   `ADEDataset`: PyTorch Dataset class for BERT fine-tuning.
-   **`utils/evaluation.py`**:
    -   `calculate_entity_metrics()`: Computes precision, recall, F1 for entity-level comparisons.
    -   `evaluate_bio_clinicalbert_model()` (in `run_all_bio_BERT.py`): Uses `seqeval` for token-level BERT evaluation.
    -   `visualize_results()`: Generates plots for comparison.
-   **`utils/utils.py`**: General helper functions for file I/O, error handling, etc.

## Requirements

The main dependencies are listed in `requirements.txt` and include:
-   `torch`
-   `transformers`
-   `datasets`
-   `evaluate` (for `seqeval`)
-   `scikit-learn`
-   `pandas`, `numpy`
-   `matplotlib`, `seaborn` (for plotting)
-   `openai`
-   `dspy-ai`
-   `litellm`
-   `python-dotenv`

## Future Work / Potential Enhancements
- Implement more sophisticated DSPy optimizers (e.g., `BootstrapFewShotWithRandomSearch`).
- Add relation extraction capabilities and evaluation.
- Experiment with different base BERT models or other transformer architectures.
- Integrate more comprehensive error analysis.
- Expand the gold standard dataset.

## Contributing

Contributions, issues, and feature requests are welcome!

## License

Specify your project's license here (e.g., MIT, Apache 2.0).
